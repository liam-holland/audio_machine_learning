{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfa9afa-00a8-4449-be71-6330244042d7",
   "metadata": {},
   "source": [
    "# Audio Machine Learning\n",
    "## Worksheet - Linear Regression and Gradient Descent\n",
    "-----\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this worksheet you‚Äôll fit a linear regression model to room acoustics data using gradient descent. You‚Äôll work with the dataset RoomT60s.csv, where:\n",
    "\n",
    "- Feature (input): Room Volume\n",
    "- Target (label): Room reverberation time T60s\n",
    "\n",
    "### Goal\n",
    "\n",
    "By the end of this worksheet, you should be able to:\n",
    "\n",
    "- Load a dataset from a CSV file and separate features and targets\n",
    "- Apply the Linear Model to data using NumPy\n",
    "- Measure model performance using a loss function (MSE, optionally MAE)\n",
    "- Implement batch gradient descent to learn $\\theta$ by minimising MSE\n",
    "- Visualise results: data + fitted line, and loss vs training iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044fb8f-ef17-4425-abe5-594a250b73b5",
   "metadata": {},
   "source": [
    "# 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e6778-0ffa-4010-b13b-9b89f120dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Import ALL the libraries we will use python -m pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402c630-c5a6-4a4c-bf5c-a07cd84a39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions for later use\n",
    "\n",
    "def MSE_loss(y_, y_hat_):\n",
    "    assert y_.shape == y_hat_.shape, f'y and y_hat shapes should be equal, but y.shape = {y.shape} and y_hat.shape = {y_hat.shape}'\n",
    "    return np.mean((y_ - y_hat_)**2)\n",
    "\n",
    "def MAE_loss(y_, y_hat_):\n",
    "    assert y_.shape == y_hat_.shape, f'y and y_hat shapes should be equal, but y.shape = {y.shape} and y_hat.shape = {y_hat.shape}'\n",
    "    return np.mean(np.abs(y_ - y_hat_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa412d-a05e-4d9a-be9b-ac0f99b11db3",
   "metadata": {},
   "source": [
    "# 1 - Working with Data using Pandas \n",
    "\n",
    "We‚Äôll start by generating a small synthetic dataset.  \n",
    "Later, you‚Äôll load a dataset from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c19bd7-9375-479b-9674-7a69767e715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 50) # Create our input variable, x\n",
    "y = 2.5 * x + 1.0 + np.random.normal(0, 2, size=len(x)) # Create our target variable, y\n",
    "\n",
    "# Put into a pandas DataFrame\n",
    "df = pd.DataFrame({\"Input Variable\": x, \"Target Variable\": y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669097b-c144-4488-82ae-af0b86779a7b",
   "metadata": {},
   "source": [
    "The Pandas DataFrame is a convinient Python object for working with tabular data.\n",
    "`df.head()` displays the first five entries in the Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313bebc-0bef-4f6b-bb4e-45e24b835279",
   "metadata": {},
   "source": [
    "### 1.1 Splitting Features and Target\n",
    "\n",
    "We‚Äôll separate the input (`input variable`) from the target (`target variable`).  \n",
    "Scikit-learn expects the input variables (or, *features*) to be in **2D array** form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5baecf-ab10-4d73-9bc4-d245b384d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = df[[\"Input Variable\"]]   # double brackets -> keeps 2D shape\n",
    "y_values = df[\"Target Variable\"]     # single bracket -> 1D Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5af635-7b04-4011-a0d0-0d73a2f6dd71",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úèÔ∏è‚úèÔ∏è Exercise ‚úèÔ∏è‚úèÔ∏è\n",
    "---\n",
    "\n",
    "This created two smaller DataFrames, `x_values` and `y_values`. \n",
    "You can access the values as a NumPy array, using the `.values` attribute.\n",
    "\n",
    "Use the `.values` attribute to determine:\n",
    "- How many data entries are there for x and for y?\n",
    "- What is the number of dimensions in the `x_values` array?\n",
    "- What is the number of dimensions in the `y_values` array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38eb3fb-a194-480a-b9f8-97b2241e6b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daa746a6-58b6-4069-b3fe-fe697230da22",
   "metadata": {},
   "source": [
    "# 2 - Load + Inspect Data\n",
    "The above code created a dataset using NumPy. Now we will load the dataset from a .csv file called 'RoomT60s.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cc1fc-ae30-45ff-b830-3276122637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"RoomT60s.csv\")\n",
    "display(df.head())\n",
    "print(df.columns)\n",
    "print(\"Rows:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58539b61-2d61-46bd-9bd2-9d2428014031",
   "metadata": {},
   "source": [
    "We can extract the 'features' and 'labels' from the above pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e630a0-241d-4bcd-9f08-8c51a0663ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_series = df[\"Volume\"]    # (N)\n",
    "y_series = df[\"T60s\"]      # (N,)\n",
    "\n",
    "x = x_series.values        # (N)\n",
    "y = y_series.values        # (N,)\n",
    "\n",
    "print(\"x:\", x.shape, \"ndim:\", x.ndim)\n",
    "print(\"y:\", y.shape, \"ndim:\", y.ndim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ec10d-2eed-4a07-adcd-9363a8a5f73c",
   "metadata": {},
   "source": [
    "- Feature: Volume ($x$)\n",
    "- Target: T60 ($y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a39cbd-6392-4840-bfc9-16a6b33daa4a",
   "metadata": {},
   "source": [
    "# 3 - Plot Data + Initialise Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1851a-09a1-482c-90c5-8a2970c94b21",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úèÔ∏è‚úèÔ∏è Exercise ‚úèÔ∏è‚úèÔ∏è\n",
    "---\n",
    "\n",
    "Plot the data. Compare to the jpg 'Vol_vs_T60.jpg' that was included with the Worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a496a-bf19-4855-8a50-dd6680bfdf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e165f296-a356-4ad2-9e82-8a8a5a50acc9",
   "metadata": {},
   "source": [
    "----\n",
    "### 3.1 - Apply the Linear Model \n",
    "Choose initial values for the linear model parameters, $m$ and $c$.\n",
    "\n",
    "The model is defined as:\n",
    "    \n",
    "$$\\hat{y} = mx + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42646a18-195b-419d-ad60-2779141c56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.0001 \n",
    "c = 0.0\n",
    "y_hat = m*x + c  #This applies the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb85bd9-6798-4e48-bda3-64949a7626e6",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úèÔ∏è‚úèÔ∏è Exercise ‚úèÔ∏è‚úèÔ∏è\n",
    "---\n",
    "\n",
    "Do the following:\n",
    "- Create a new plot of the data, including the above linear model predictions.\n",
    "- Confirm that the number of predicitions contained in `y_hat` equals the number of target values in `y`\n",
    "- Find the Mean Absolute Error over the dataset\n",
    "- Find the Mean Squared Error over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91883373-b3f8-4f2e-b732-5556bfc589b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8638b897-b930-41d9-afe7-d1ae4d976853",
   "metadata": {},
   "source": [
    "# 4 - Visualise Loss Surface\n",
    "\n",
    "We can visualise the loss surface, by plotting possible values of the parameters, $m$ and $c$, against the loss.\n",
    "\n",
    "---\n",
    "### ‚úèÔ∏è‚úèÔ∏è Exercise ‚úèÔ∏è‚úèÔ∏è\n",
    "---\n",
    "\n",
    "Complete the code below, such that for each 'candidate value' of $m$, we have the correponding loss value over the full dataset. The result should be an array of values for the parameter $m$, and a corresponding array `losses`,  holding the resulting model predicition. so, for example:\n",
    "\n",
    "`y_hat[0] = ms[0]*x + c`\n",
    "\n",
    "and \n",
    "\n",
    "`y_hat[1] = ms[1]*x + c`\n",
    "\n",
    "and so on.\n",
    "\n",
    "Then, plot the resulting array of losses against the candidate values for m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a3e42-b81e-4572-8af0-f93f9074ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.linspace(-0.1, 0.1, 50) # Create a vector of 'candidate values' for m\n",
    "\n",
    "# Create an empty list to hold the loss values\n",
    "losses = []\n",
    "for candidate_m in ms: # Iterate through the candidate values of m\n",
    "    print(candidate_m) # For information, comment this out!\n",
    "    \n",
    "    # 1 - Write code here applying the linear model to create the predictions, y_hat\n",
    "    \n",
    "    # 2 - Calculate the MSE loss for this set of predictions\n",
    "\n",
    "    # 3 - Append the loss to the list 'losses'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003c30f-37c3-486d-94e6-464fc71856e5",
   "metadata": {},
   "source": [
    "----\n",
    "The following code plots the result. It should like something like the jpg 'MSE_vs_m' included with the worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a9807-9c90-48c9-9552-4fa6aab4cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)  # Create a NumPy array from the list of losses\n",
    "plt.plot(ms, losses)       # Plot the candidate values for m against the losses\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('MSE Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897814e2-7d21-473f-8d2c-e2372ef47b4b",
   "metadata": {},
   "source": [
    "From the plot, you can see the loss as a function of the model parameter $m$:\n",
    "- In this case there is a **global minimum** of the loss with respect to the model parameter $m$.\n",
    "- The value for $m$ which minimises the loss can seen on the graph, as the bottom of the 'bowl' shape.\n",
    "- In gradient descent, we choose a starting point for the model parameters, then adjust them in the direction that causes the loss to decrease\n",
    "- We do this by finding the **gradient** of the loss function with respect to the model parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46c755-2085-4451-9f4d-22e63248d6ca",
   "metadata": {},
   "source": [
    "# 5 - Gradient Descent for a Single Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecd391-aba0-41c8-acc9-ff566f484078",
   "metadata": {},
   "source": [
    "We will start by finding the gradient of the loss with respect to the model parameters, $m$ and $c$, for a single training example $i$.\n",
    "\n",
    "For one example $i$:\n",
    "\n",
    "- **Prediction:**  \n",
    "  $\\hat{y}^{(i)} = m x^{(i)} + c$\n",
    "\n",
    "- **Model Error:**  \n",
    "  $r^{(i)} = y^{(i)} - \\hat{y}^{(i)}$\n",
    "\n",
    "- **Per-example squared loss:**  \n",
    "  $\\ell^{(i)} = (y^{(i)} - (mx^{(i)} + c))^2$\n",
    "\n",
    "Gradients of the loss with respect to the parameters, for the example $i$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\ell^{(i)}}{\\partial m} = -2 r^{(i)} x^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial \\ell^{(i)}}{\\partial c} = -2 r^{(i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196cbfa-fd4b-439b-b914-8ab2b5adc97d",
   "metadata": {},
   "source": [
    "The below function takes a single training example from the dataset, as well as model parameters $m$ and $c$, and returns the gradient of the loss with respect to the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32323a-b3aa-416c-a7f3-0a1b4e3475ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_example_grads(x_i_, y_i_, m_, c_):\n",
    "    # Check only a single example has been passed into the function\n",
    "    assert y_i_.size == 1, 'y_i_ has multiple elements, this function is for single example only'\n",
    "    # Apply linear model, and find error\n",
    "    y_hat_i = m_*x_i_ + c_\n",
    "    r_i = y_i_ - y_hat_i\n",
    "    # Find gradient of error wrt to parameters m and c\n",
    "    dli_dm = -2 * r_i * x_i_\n",
    "    dli_dc = -2 * r_i\n",
    "    return dli_dm, dli_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9811-07d3-41c7-8548-89f2cc0a02a3",
   "metadata": {},
   "source": [
    "### 5.1 - Loss and Gradient Calculation\n",
    "We can calculate the squared error for a single example, then find the gradient of the loss wrt to each model parameter using the `per_example_grads` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec52f9-8f15-4507-b5cb-06e38ccc0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with candidate model parameters\n",
    "m = 0.0\n",
    "c = 0.0\n",
    "\n",
    "# Use the first training example from the dataset\n",
    "i = 0\n",
    "\n",
    "# Apply the linear model and find the squared error, sq\n",
    "y_hat_i = m*x[i] + c\n",
    "sq = (y[i] - y_hat_i)**2\n",
    "print(f'Model parameters: m = {m}, c = {c}')\n",
    "print(f'Squared error for example i = {i}: {sq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5604b-964b-491c-b1c7-7b9087865d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradient of the loss with respect to m and c \n",
    "grads = per_example_grads(x[i], y[i], m, c)\n",
    "print(f'dli_dm = {grads[0]}, dli_dc = {grads[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74fe75-1c7d-43d1-9092-ac9227a86230",
   "metadata": {},
   "source": [
    "A positive gradient indicates that increasing the parameter value will increase the loss. Likewise, a negative gradient indicates that increasing the parameter value will decrease the loss. \n",
    "\n",
    "When carrying out gradient descent, we adjust the parameters in the opposite direction to the gradient. You can think of this as 'rolling down the slope' shown in the 'MSE_vs_m.jpg' plot from earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f9cf7-4a74-453b-8ea7-1f11ff5161b0",
   "metadata": {},
   "source": [
    "### 5.2 - Gradient Descent Step\n",
    "\n",
    "Let's take a single step of gradient descent, for our single example, $i$:\n",
    "\n",
    "$m:= m - \\alpha \\frac{\\partial \\ell^{(i)}}{\\partial m}$\n",
    "\n",
    "$c:= c - \\alpha \\frac{\\partial \\ell^{(i)}}{\\partial c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7394f-fd88-413d-b994-f9573c9458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate alpha\n",
    "alpha = 0.00000001\n",
    "assert alpha > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c370f-2000-442f-bf08-f6fa2edb12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial model parameters: m = {m}, c = {c}')\n",
    "print(f'Squared error for example i = {i}: {sq}')\n",
    "\n",
    "grads = per_example_grads(x[i], y[i], m, c)\n",
    "m = m - alpha*grads[0]\n",
    "c = c - alpha*grads[1]\n",
    "y_hat_i = m*x[i] + c\n",
    "sq = (y[i] - y_hat_i)**2\n",
    "\n",
    "print(f'Upated model parameters: m = {m}, c = {c}')\n",
    "print(f'Updated squared error for example i = {i}: {sq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb2f2e-793c-4954-b479-c390f8faa81b",
   "metadata": {},
   "source": [
    "The squared error after updating the parameters should be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8402a-f169-4c39-a54a-1c0bcdfd12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.linspace(-0.01, 0.01, 20) # Create a vector of 'candidate values' for m\n",
    "losses = [] # Create a list to hold the predictions\n",
    "\n",
    "for candidate_m in ms: # This is a 'for loop', it iterates through the items in 'ms'\n",
    "    print(candidate_m)\n",
    "    \n",
    "    # Write code here, so that for every 'candidate_m', the model is applied to the example x[i] \n",
    "    # to find the model prediciton\n",
    "\n",
    "    #Your Code Here!\n",
    "    \n",
    "    # Use the .append() function to append the loss to the list of losses\n",
    "\n",
    "    #Your Code Here!\n",
    "\n",
    "    \n",
    "# This creates the array of losses\n",
    "losses = np.array(losses)\n",
    "\n",
    "\n",
    "# Now plot the losses against the candidate_m values\n",
    "#Your Code Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54150d1-6b75-4984-a6b6-a4bbd53fe600",
   "metadata": {},
   "source": [
    "The resulting plot should look something like the file 'MSE_vs_m.jpg' included with this worksheet. When we carry out gradient descent, we are trying to find the parameter values correponding to the bottom of bowl-like shape shown in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df78284-823b-49a7-8bba-12670785ad00",
   "metadata": {},
   "source": [
    "# 6 - Gradient Descent for Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc810f0-94d7-4c5b-972f-17be0f815087",
   "metadata": {},
   "source": [
    "Batch gradients are the mean of per-example gradients:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m}\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial \\ell^{(i)}}{\\partial m}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial c}\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial \\ell^{(i)}}{\\partial c}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23364e9-5479-4d26-9690-30381cb7b700",
   "metadata": {},
   "source": [
    "The function `batch_grads_mean` finds the mean gradient over a batch of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3edcd-42ee-40de-a986-71719691938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grads_mean(x_, y_, m_, c_):\n",
    "    # Create lists to hold gradients for each example\n",
    "    grads_m = []\n",
    "    grads_c = []\n",
    "    # Iterate over each example\n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        dli_dm, dli_dc = per_example_grads(x_i, y_i, m_, c_)\n",
    "        grads_m.append(dli_dm)\n",
    "        grads_c.append(dli_dc)\n",
    "    # Find mean gradient over batch\n",
    "    dL_dm = np.mean(grads_m)\n",
    "    dL_dc = np.mean(grads_c)\n",
    "    return dL_dm, dL_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44c2c3-8a64-424a-be1f-7dd649c61bb1",
   "metadata": {},
   "source": [
    "## 6.1 - A Step of Gradient Descent\n",
    "\n",
    "Let's take a single step of gradient descent, for our complete dataset:\n",
    "\n",
    "$$m:= m - \\alpha \\frac{\\partial L}{\\partial m}$$\n",
    "\n",
    "$$c:= c - \\alpha \\frac{\\partial L}{\\partial c}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1eeafe-73d9-44d3-bf73-c8dc4bf33f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate alpha\n",
    "alpha = 0.00000001\n",
    "# Reset the model parameters\n",
    "m, c = 0, 0\n",
    "\n",
    "y_hat_before = m*x + c\n",
    "loss_before = MSE_loss(y, y_hat_before)\n",
    "\n",
    "dL_dm, dL_dc = batch_grads_mean(x, y, m, c)\n",
    "m_new = m - alpha*dL_dm\n",
    "c_new = c - alpha*dL_dc\n",
    "\n",
    "y_hat_after = m_new*x + c_new\n",
    "loss_after = MSE_loss(y, y_hat_after)\n",
    "\n",
    "print(f'Loss before update: {loss_before}')\n",
    "print(f'Loss_after update: {loss_after}')\n",
    "print(f'Parameters after update: m = {m_new}, c = {c_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec0ef0-ebf7-4432-9f2e-39c07fd82420",
   "metadata": {},
   "source": [
    "## 6.2 - Plotting the Result\n",
    "\n",
    "We can visualise the model before and after the parameter update, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198b40d-362e-46ee-9350-221c08113fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=10, alpha=0.4, label=\"data\")\n",
    "\n",
    "xx = np.linspace(x.min(), x.max(), 200)\n",
    "plt.plot(xx, m*xx + c, label=\"before update\")\n",
    "plt.plot(xx, m_new*xx + c_new, label=\"after 1 update\")\n",
    "\n",
    "plt.xlabel(\"Volume (m^3)\")\n",
    "plt.ylabel(\"T60 (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435a8d8-1ab5-4427-8ec8-4792da124258",
   "metadata": {},
   "source": [
    "# üéØüéØ Challenge! üéØüéØ\n",
    "\n",
    "The above code implements a single step of batch gradient descent for the linear model. \n",
    "\n",
    "Your task now is to turn that into a full training loop, that repeats gradient descent for many iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step instructions\n",
    "\n",
    "1. Choose hyperparameters and initial values\n",
    "   - Use the learning rate `alpha` defined earlier, and choose the number of training `iterations`\n",
    "   - Initialise the model parameters:\n",
    "     - m = 0.0\n",
    "     - c = 0.0\n",
    "2. Create history lists to monitor model learning\n",
    "   - Make empty lists to monitor the values of the loss, `m` and `c`\n",
    "     - `loss_history`\n",
    "     - `m_history`\n",
    "     - `c_history`\n",
    "   - These will store values at each iteration\n",
    "3. Write the training loop\n",
    "   - This is a `for-loop` that runs for `iterations` steps.\n",
    "   - Each iteration should:\n",
    "     - *Forward pass*: compute predicitions `y_hat` for all examples\n",
    "       - `y_hat = mx + c`\n",
    "     - *Compute Loss*: compute the MSE for the dataset and append to `loss_history`\n",
    "     - *Store Parameters*: append the current values of `m` and `c` to `m_history` and `c_history`\n",
    "     - *Compute Gradients*: use the `batch_grads_mean` function to find the gradients of the loss with respect to the parameters `m` and `c`\n",
    "     - *Update Parameters*: perform gradient descent as in section 6.1\n",
    "     - *Print Progress* (optional): every 50 iterations, print the loss, and the parameters, so you can monitor model training progress\n",
    "4. Check Results\n",
    "   - After training create plots of:\n",
    "     - The loss vs training iteration number\n",
    "     - The model prediction line at the start and end of training, along with a scatter plot of the training data\n",
    "   - The loss should generally be decreasing with each iteration\n",
    "   - The final model line should look like a reasonable fit when plotted over the training data\n",
    "   - If your loss becomes `nan` (not a number), or explodes/becomes unstable, try:\n",
    "     - reducing learning rate `alpha`\n",
    "     - scale `x` to be smaller\n",
    "\n",
    "Below is a code template to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5403a1-4753-4ddb-9429-79a9c4a4b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = ...          # TODO (e.g. 0.000000001)\n",
    "iterations = ...     # TODO (e.g. 200 or 500)\n",
    "\n",
    "\n",
    "# Initial parameters\n",
    "m, c = ...    # TODO\n",
    "\n",
    "# Lists for monitoring training\n",
    "# TODO - create empty lists for monitoring m, c and loss over each iteration\n",
    "\n",
    "# Iterate\n",
    "for it in range(iterations):\n",
    "\n",
    "    # 1) Forward pass (predictions)\n",
    "    y_hat = ...  # TODO: vector of predictions for all x, using the current values of m and c\n",
    "\n",
    "    # 2) Calculate Loss\n",
    "    loss = ...   # TODO: Find the loss\n",
    "    loss_history... # TODO: append the loss to the loss_history list\n",
    "\n",
    "    # 3) Record parameters\n",
    "    # TODO: Append the parameters m and c to the m_history and c_history lists\n",
    "\n",
    "    # 4) Get gradients (mean of per-example gradients)\n",
    "    #dm, dc = ...  # TODO: call the batch gradient function\n",
    "\n",
    "    # 5) Parameter update (gradient descent step)\n",
    "    #m = ...       # TODO: update m using alpha and dm\n",
    "    #c = ...       # TODO: update c using alpha and dc\n",
    "\n",
    "    # 6) Optional debug printing\n",
    "    if it % 50 == 0:\n",
    "        print(loss)\n",
    "        pass #TODO print some information about the training\n",
    "\n",
    "print(\"Final parameters:\", m, c)\n",
    "print(\"Final loss:\", loss_history[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe4c4a-9958-4c7b-af65-5809b4c4aca7",
   "metadata": {},
   "source": [
    "Now create the plots described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b97bad-6ab8-4f41-bc32-fc847e2e6ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2feaade6-9afb-4768-a07d-7fe7cc54a621",
   "metadata": {},
   "source": [
    "## Further Work\n",
    "\n",
    "If you wish to go further, here are some ideas for things you can try (in no particular order!):\n",
    "\n",
    "- Implement Stochastic Gradient Descent (SGD): Update `m` and `c` using one training example at a time (optionally shuffle the data each epoch). Compare the loss curve to batch Gradient Descent (An epoch = one full pass through the dataset).\n",
    "  \n",
    "- Implement Mini-Batch Stochastic Gradient Descent: Update `m` and `c` using a small batch of randomly selected training examples at a time (e.g. batch size 8, 16, or 32).\n",
    "\n",
    "- Experiment with the learning rate `alpha`: Try values that are smaller/larger and describe what you observe (slow convergence, oscillation, divergence). Find a ‚Äúgood‚Äù range for your data. In practice, you would usually scale the features `x` first (e.g. min‚Äìmax scale to [-1, 1], or standardise to zero mean and unit variance). Try this and see how it changes the suitable range of alpha.\n",
    "\n",
    "- Rewrite the model in matrix form: In the lecture slides, we applied the model using matrix multiplication, which makes it easier to add additional features. Rewrite the model so predictions are computed as $\\hat{y} = X\\theta$ (you will need to add a bias column of ones to X). Use the '@' operator for matrix multiplication. Confirm this implementation of the model gives the same results as the `y_hat = mx + c` version used in this worksheet.\n",
    "\n",
    "- Add a second feature: Try adding a second feature to the model, you can use 'absorption' from the RoomT60s.csv dataset, for example. This will mean there are now three parameters in the model: $$\\hat{y} = \\theta_0 * Volume + \\theta_1 * absorption + \\theta_2$$ Compare the performance of this with the single-feature model.\n",
    "\n",
    "- Vectorise the gradient using broadcasting (no Python loop): The `batch_grads_mean` function currently uses a for-loop to find the mean of the gradients over the dataset, which is inefficient. Compute all per-example errors at once, then take means to get dm and dc. Check correctness by comparing your vectorised gradients to the for-loop version of the gradients (e.g. using `np.allclose`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fa16e-e34a-4c74-8c02-b6e0334b5b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
