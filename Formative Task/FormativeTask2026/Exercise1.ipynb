{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b83e5c-78e3-4ec1-8dc5-585e16484d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdf1df-c822-4c38-bbb4-3dbf65a4f1c5",
   "metadata": {},
   "source": [
    "# Audio Machine Learning - Formative Task - Exercise 1\n",
    "## 1 - k-Nearest Neighbour Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead01a4-d6ab-4dba-a583-91c6e09db35f",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1c937-29cd-4bf4-a774-3da79d56b32e",
   "metadata": {},
   "source": [
    "The dataset $D = \\{(x^{(i)}, y^{(i)})\\}^N_{i=1}$, contains $N$ labelled datapoints. Each datapoint consists of a feature vector, $x^{(i)}$, and the correspoinding class label, $y^{(i)}$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab14ab6-793c-4f51-8237-311390d34aeb",
   "metadata": {},
   "source": [
    "$x^{(i)}$ denotes the feature vector of the i-th datapoint is the dataset, and $y^{(i)}$ denotes the i-th label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51515c-3d7c-4fc6-a433-15c2bf228c1c",
   "metadata": {},
   "source": [
    "Each feature vector $x^{(i)}$, consists of $j$ features. $x^{(i)}_j$ denotes the j-th element in the feature vector $x^{(i)}$. Each class label, $y^{(i)}$, consists of a single class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0dacf-c5f8-4cfb-b6d9-01f4e427b1ea",
   "metadata": {},
   "source": [
    "### 1.2 - Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b4e4c-9a3a-4308-bcea-87cacf728044",
   "metadata": {},
   "source": [
    "We want to classify a new datapoint, $q$, which consists of $j$ features but for which the class label is unknown. We can approach this by measuring the euclidean distance between $q$ and each of the $N$ datapoints in the dataset $D$. The euclidean distance between $q$ and a feature vector $x^{(i)}$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe011f3c-c7fe-4598-8ff0-adf6369cb759",
   "metadata": {},
   "source": [
    "Euclidean Distance = $\\sqrt{\\sum\\limits_{n=1}^{j} (x^{(i)}_n - q_n)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7cd7ff-5dbb-4710-b5ea-14f4f5616dca",
   "metadata": {},
   "source": [
    "Once we've calculated the distance between $q$ and each of the $N$ datapoints in the dataset, we can rank each datapoint $x^{(i)}$ by it's distance to the query point $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00475b-d7d5-4b94-bfc1-d5731661d145",
   "metadata": {},
   "source": [
    "### 1.3 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e703c87-6d65-4acb-bfdf-6a63f7176014",
   "metadata": {},
   "source": [
    "To carry out classification, the $k$ datapoints that are closest to the query $q$ are selected, where $k$ is a user-defined constant. The most frequently occuring label amongst the selected $k$ datapoints is the k-nearest neighbour algorithm's predicted label for the query $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145444f-c113-4431-a1d7-4bcec4160896",
   "metadata": {},
   "source": [
    "## 2 - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c2eef-d109-4bd7-99dc-3e9b6bf90716",
   "metadata": {},
   "source": [
    "### 2.1 - Task 1 - Implementing the KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc12d0-9ba3-4bbd-a993-e745951947ac",
   "metadata": {},
   "source": [
    "Your task is to implement the k-nearest neighbour classifier, using the Euclidean Distance. It should work for a dataset with any number of training examples, N, and any number of features.  You should use the Python class template provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8cf47d9-e89a-4cf4-a031-3d134394258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\_'\n",
      "C:\\Users\\Liam Holland\\AppData\\Local\\Temp\\ipykernel_20624\\4213540141.py:18: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  features\\_(np.array): Feature array with shape (N x j)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define the KNNClassifier class\n",
    "class KNNClassifier:\n",
    "\n",
    "    # The KNNClassifier should have the method 'fit', which takes the features and labels from Dataset D as it's arguments and saves them for classification.\n",
    "    # If the KNNClassifier already has a dataset saved, it should be replaced by whatever dataset is passed to the 'fit' method\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # features_ should be a numpy array of shape (N, j)\n",
    "    # labels_ should be a numpy array of shape (N)\n",
    "\n",
    "    def fit(self, features_, labels_):\n",
    "        \"\"\" \n",
    "        \n",
    "        Args:\n",
    "            features\\_(np.array): Feature array with shape (N x j)\n",
    "            labels\\_(np.array): Label array with shape (N)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "               \n",
    "        self.features = np.array(features_)\n",
    "        self.labels = np.array(labels_)\n",
    "\n",
    "        self.shape_features = np.shape(self.features)[1]\n",
    "        self.dimFeatures = np.ndim(self.features)\n",
    "\n",
    "        if self.dimFeatures != 2:\n",
    "            raise ValueError( f\"Expected 2D array (N, j), got {features_.ndim}\")\n",
    "        \n",
    "        if len(self.features) != len(self.labels):\n",
    "            raise ValueError( f\"Features and labels must have the same number of rows\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "        \n",
    "    # The KNNClassifier should have the method 'predict', which takes a query vector q, and the parameter k. The 'predict' method should implement \n",
    "    # the KNN algoirthm, returning the classifier's predicted class label for query q. If an invalid query is made, i.e the number of features in q is\n",
    "    # incorrect, the predict function should inform the user.\n",
    "    def predict(self, q, k):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            q (np.array): The query vector with shape (1,j)\n",
    "            k (int): Number of neighbors\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            str: The predicted label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if np.shape(q) != (1,self.shape_features):\n",
    "            return f\"q has the incorrect dimensions ( {np.shape(q)} ), the correct dimensions are {self.shape_features}\"\n",
    "               \n",
    "        self.q = np.array( q ) # This is our query. The new data point which we are trying to guess\n",
    "        self.k = k # This is the number of nearest neighbours we want to use in our model\n",
    "\n",
    "        distance =   np.sqrt( np.sum((self.features - q)**2,axis=1) )\n",
    "        arg_order =  np.argpartition(distance, k)[:k]   # Memorize the sort order, argpartition gives me back the k smallest, but doesn't worry about the order\n",
    "\n",
    "        # Find the labels of the k nearest neighbours\n",
    "        label_sort_k = self.labels[arg_order]\n",
    "\n",
    "        # Return the most common value in our sorted list\n",
    "        best_label = Counter(label_sort_k).most_common(1)\n",
    "\n",
    "        # best_label returns a string, so need to access it with [0][0]\n",
    "        return best_label[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf2f33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best label for q is 'white'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feat = [[1, 4],[2,4],[1,2],[4,5]]\n",
    "labels = [\"white\",\"black\",\"white\",\"white\"]\n",
    "q = [[1,3]]\n",
    "\n",
    "intialise = KNNClassifier()\n",
    "\n",
    "fit_model = intialise.fit(features_= feat , labels_= labels) \n",
    "\n",
    "predict_model = fit_model.predict(q,3)\n",
    "\n",
    "print(predict_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdfa1a-7475-421a-9eac-2417e522dab3",
   "metadata": {},
   "source": [
    "You should aim to implement the predict function without using any 'for' loops.\n",
    "\n",
    "All methods should have clear comments describing what each line of code does. The methods should also have comments describing the expected data types\n",
    "and dimensions of the input arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9feccd-837c-44e8-9dd5-d39613966c17",
   "metadata": {},
   "source": [
    "## 3 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d0508-8ee3-4f91-80db-0669f17dc53d",
   "metadata": {},
   "source": [
    "### 3.1 - Testing the KNN Classifier\n",
    "Section 3.1 contains code to generate training examples and test your classifier. You don't have to implement anything here but you may find it useful for testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b46e7-cc8b-4687-91b5-012290797dbb",
   "metadata": {},
   "source": [
    "The below functions generate data from two different classes, as well as the corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111ccc0e-47d7-4b0b-bff7-923abb5826dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions generate data from two different classes\n",
    "def class_1_generator(num_samples):\n",
    "    return np.random.randn(num_samples, 2) + 2.5, np.ones(num_samples, dtype=np.uint32)\n",
    "\n",
    "def class_2_generator(num_samples):\n",
    "    return np.random.randn(num_samples, 2) + 5, 2*np.ones(num_samples, dtype=np.uint32)\n",
    "    \n",
    "# This function generates a dataset, and return a tuple (X, Y), where X contains N datapoints each consisting of j features, shape [N, j], and Y contains the\n",
    "# corresponding class labels, shape [N]. The argument 'examples_per_class' determines how many datapoints from each class to include in the dataset.\n",
    "def generate_dataset(examples_per_class):\n",
    "    class_1s = class_1_generator(examples_per_class)\n",
    "    class_2s = class_2_generator(examples_per_class)\n",
    "    dataset = (np.concatenate([class_1s[0], class_2s[0]]), np.concatenate([class_1s[1], class_2s[1]]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2c0a1-beb9-432d-983f-98ea0d212b02",
   "metadata": {},
   "source": [
    "The below code will create a Model object from your KNNClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c091542-c66e-4682-b17a-ad54b1613527",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873a80d-46fc-42d3-aac8-9bb43925f73c",
   "metadata": {},
   "source": [
    "The below code will generate a training dataset, and pass it to the KNNClassifier.fit() method, which you implemented in section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5dd723-ce77-42a9-a0c8-98ef346f14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataset = generate_dataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf7e49d1-321f-4d8f-9d65-1d9b0a62486c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KNNClassifier at 0x1dfd15519d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(features_=Train_Dataset[0], labels_=Train_Dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03337e-ff61-4e83-bd84-dfd825e67514",
   "metadata": {},
   "source": [
    "The below code generates a test dataset, and then passes a single query to KNNClassifier using the .predict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "280e7934-d446-4619-8581-4d96a0ee6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Dataset = generate_dataset(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe31d64-f341-4122-8617-8d6ca37f7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Model.predict(q=Test_Dataset[0][0:1,:], k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988335b-20f6-4ec1-b448-6e040333e210",
   "metadata": {},
   "source": [
    "Now print the prediciton of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5437fdb1-d63f-4581-8b97-a19dd73017ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for query point [3.38742632 2.78310759] is class: The best label for q is '1'. The true label is 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for query point {Test_Dataset[0][0,:]} is class: {prediction}. The true label is {Test_Dataset[1][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f3327-26bf-49e1-b0a8-bc7ea38f3cac",
   "metadata": {},
   "source": [
    "### 3.2 - Task 2 - Test your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7722d-0cf1-4223-839c-475281315da5",
   "metadata": {},
   "source": [
    "Your task is to use your KNN Classifier to classify all the examples in 'Test_Dataset' (created in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471c001-f59d-473e-8f5d-2086b1821f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "false: The prediction is 2, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 1, the actual value is 1\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "false: The prediction is 1, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "correct: The prediction is 2, the actual value is 2\n",
      "The overall accuracy is 96.0 %\n"
     ]
    }
   ],
   "source": [
    "Model = KNNClassifier()\n",
    "Model.fit(features_=Train_Dataset[0], labels_=Train_Dataset[1])\n",
    "\n",
    "q_values = np.array(Test_Dataset[0])\n",
    "label_values = np.array(Test_Dataset[1])\n",
    "\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "for q, l in zip( q_values, label_values):\n",
    "\n",
    "    # Need to make sure that q has a size of 1 x 2\n",
    "    pred = Model.predict( [q] , 5)\n",
    "    actual = l\n",
    "\n",
    "    if pred == actual:\n",
    "        a = \"correct\"\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        a = \"false\"    \n",
    "\n",
    "    print( f\"{a}: The prediction is {pred}, the actual value is {actual}\" )\n",
    "\n",
    "percentage_accuracy =  ( correct_count / np.shape(q_values)[0] ) * 100\n",
    "\n",
    "print(f\"The overall accuracy is {percentage_accuracy} %\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34984f-2012-4f0a-9d1f-315d6b5e31ee",
   "metadata": {},
   "source": [
    "You should then compare the predictions to the true class labels from 'Test_Dataset'. \n",
    "The accuracy is given by the ratio of the number of correct predictions to the total number of samples input to the classifier.\n",
    "Calculate the accuracy of the classifier, for k=1, k=2 and k=5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4394a212-a125-4013-a6de-a653ff69099d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "q_values.size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
