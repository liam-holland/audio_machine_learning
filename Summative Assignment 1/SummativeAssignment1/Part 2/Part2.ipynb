{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Noteable, run this:\n",
    "%pip install librosa\n",
    "# Otherwise, if not using Noteable, ensure you have librosa installed in your Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4990e8de92215aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273dd74-0bdb-4760-9c15-562f9a3a0a7c",
   "metadata": {},
   "source": [
    "# Python Libraries\n",
    "\n",
    "You do not need to use any Python libraries in addition to those listed above. Do not import any other libraries. If you think I have missed out a library that you are meant to use during this assignment, please contact me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920446e2adcaaca1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Audio Machine Learning - Summative Assessment 1 - Machine Learning Challenge - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7008f83db4527dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the second part of the machine learning challenge, your task is to implement and train a spoken digit classifier using PyTorch\n",
    "\n",
    "The model is intended to classify spoken digits, specifically the numbers 0 to 9 spoken in English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ff703604c247b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are sections in the Notebook which are left for you to complete, which will be assessed. This will be marked as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81106ce8b58b098",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This will be marked as below:\n",
    "\n",
    "## __Assessed Section__\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5079d0ee32e47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Any code cells in the 'Assessed Section' are part of the assessment. \n",
    "# There will be instructions for you to follow within these parts of the notebook.\n",
    "# Your submission will be the Notebook file, with the Assessed Sections filled in\n",
    "# This one doesn't count, as it is just a demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094cb89bfff5616",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "## __End of Assessed Section__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1a180-1bf8-4910-82de-08dcf4962aec",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Downloading the Dataset\n",
    "\n",
    "The dataset for this assignment is available to download from GitHub at the following URL:\n",
    "\n",
    "https://github.com/Alec-Wright/Digits\n",
    "\n",
    "\n",
    "### Downloading in Noteable\n",
    "You can download this directly into Noteable by:\n",
    "\n",
    "- Opening Noteable\n",
    "- Clicking the 'Git' menu item at the top of the window\n",
    "- Selecting 'Clone a Repository'\n",
    "- Entering the url 'https://github.com/Alec-Wright/Digits'\n",
    "- Clicking Clone\n",
    "\n",
    "More information available here if you have trouble.\n",
    "\n",
    "https://noteable.edina.ac.uk/user-guide/#up_4\n",
    "\n",
    "### Downloading to Computer\n",
    "- Go to https://github.com/Alec-Wright/Digits\n",
    "- Click the green '<> code' button\n",
    "- Click 'Download ZIP'\n",
    "- Unzip the downloaded .zip file\n",
    "\n",
    "This should download a folder called 'Digits', within which are various subfolers containing '.wav' files of human speech.\n",
    "\n",
    "Please contact me if you are unable to download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac463ce8df35c04",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "***\n",
    "## **Task 1** - Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767625169ad12d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the folder 'Digits', there is a folder called 'Data' that holds recordings of spoken digits, from various speakers, speaking the digits 0 to 9. This is from the 'Audio MNIST' dataset.\n",
    "\n",
    "Each audio file in the dataset is a '.wav' file that contains a recording of one speaker saying one of the digits. \n",
    "\n",
    "The folders are organised by speaker, so 'Data/01/' contains digits spoken by speaker '01' and 'Data/02/' contains digits spoken by speaker '02', and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdb426b2f67a16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## __Assessed Section - 2.1__\n",
    "----------\n",
    "\n",
    "Your task is to:\n",
    "- Determine how many audio files are in the dataset held in 'Digits/Data/' (including all subfolders). Save this to a variable called 'dataset_size'.\n",
    "- Use Librosa to load each file and find its length in samples. Determine the length of the longest audio file, and the shortest audio file, in samples. Save these to variables called 'longest_length' and 'shortest_length'.\n",
    "- For each of the 10 digits, count how many examples are contained in the dataset. Save this as a list to a variable called 'class_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68a8199db95644",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7a483ca516910",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6425f7326c1827",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "***\n",
    "## **Task 2** - Zero Padding and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9141a71e5241",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Each audio file is of a different length. We want them all to be the same length, for processing in batches with our neural network. We can do this by adding zeros to the end of shorter files, or by truncating longer files (cutting the end off). \n",
    "\n",
    "We then want to create a time-frequency representation of the audio, which we will extract using Librosa.\n",
    "\n",
    "Write a function, 'adjust_length', that adjusts the length of some input audio data. It should have the following arguments:\n",
    "- audio_data: a tensor of shape $(t,)$, where $t$ is the length of the audio in samples\n",
    "- target_length: an Integer that describes the target length in samples of the audio\n",
    "\n",
    "The function should:\n",
    "- Either apply zero-padding or truncate the audio data such that it is of shape (target_length,)\n",
    "- The function should then return the input audio, which is now of the shape  (target_length,)\n",
    "\n",
    "\n",
    "Write another function, 'preprocess', that normalises some input audio data, and then creates a time-frequency representation of it. It should take the following arguments:\n",
    "- audio_data: a numpy array of shape $(t,)$, where $t$ is the length of the audio in samples\n",
    "- sample_rate: An integer that represents the sample rate of audio_data\n",
    "\n",
    "The function should:\n",
    "- Normalise the audio data. Multiply it by some scalar such that np.max(np.abs(audio_data)) = 1.\n",
    "- Extract the mel spectrogram of the audio_data using Librosa. Use the librosa.feature.melspectrogram() function, ensuring to provide the correct sample rate to the function. You can use the default parameters for melspectrogram().\n",
    "- Return the extracted mel-spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccd3cfe6efe213",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## __Assessed Section - 2.2__\n",
    "----------\n",
    "\n",
    "Your task is to:\n",
    "- Implement the two functions described above, in the templates below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389e4c76f44aef9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def adjust_length(audio_data, target_length):\n",
    "    ## Implement the function here\n",
    "    pass\n",
    "    return 0 # Replace 0 with whatever you want this function to return\n",
    "\n",
    "def preprocess(audio_data, sample_rate):\n",
    "    ## Implement the function here\n",
    "    pass\n",
    "    return 0 # Replace 0 with whatever you want this function to return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331adb385ba3065",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.2__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c297950aa5d6dc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "***\n",
    "## **Task 3** - Creating the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3076569b283b24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will create a dataset using the recordings from speaker '01'. The audio from speaker 01 is contained in the folder 'Data/01/'\n",
    "\n",
    "Each audio file is named in the following format:\n",
    "\n",
    "'digit_speaker_i.wav'\n",
    "\n",
    "What appears before the first underscore is the digit being spoken.\n",
    "What appears after the first underscore is the speaker identity.\n",
    "What appears after the second underscore is the index.\n",
    "\n",
    "So:\n",
    "\n",
    "    '0_01_0.wav'\n",
    "\n",
    "is the digit '0'\\\n",
    "spoken by speaker '01'\\\n",
    "and it is the first recording of speaker '01' saying '0' in the dataset.\n",
    "\n",
    "    '0_01_1.wav'\n",
    "\n",
    "is the digit '0'\\\n",
    "spoken by speaker '01'\\\n",
    "and it is the SECOND recording of speaker '01' saying '0' in the dataset.\n",
    "\n",
    "and so on.\n",
    "\n",
    "\n",
    "The dataset will be split into three subsets. For each digit, the first 40 examples of that digit should be placed in the _training_ dataset. The next 5 examples should be placed in the _validation_ dataset, and the final 5 should be placed in the _test_ dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7d9ecf42e66d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## __Assessed Section - 2.3__\n",
    "----------\n",
    "\n",
    "Your task is to:\n",
    "- Create the three data subsets as described above\n",
    "- Create a list of the filenames of the audio to be contained in each subset\n",
    "- For each file:\n",
    "    - Load the audio using Librosa, resampling to a sample_rate of 22050\n",
    "    - Use the adjust_length function you defined earlier to adjust the length of the audio data to exactly 1-second\n",
    "    - Use the preprocess function you defined earlier to extract the time-frequency representation\n",
    "    - Determine the label of the file from the filename (using the filename.split('_') method might be useful here)\n",
    "- For each subset:\n",
    "    - Extract the time-frequency representation of all the audio files contained in that subset, as well as the labels\n",
    "    - Create a PyTorch tensor that holds all the time-frequency representations of the data in that subset. Ensure it is of datatype 'float32'. It should be a tensor of shape (N, C, T), where N is the number of datapoints in that subset, C is the number of frequency bins in the mel-spectrogram, and T is the length of the mel-spectrogram in frames.\n",
    "    - Create a PyTorch tensor that holds the labels corresponding to each of the N examples held in the tensor of features. It should be of shape (N,)\n",
    "\n",
    "Save the features to the variables:\n",
    "    'train_features', 'val_features' and 'test_features'\n",
    "\n",
    "Save the labels to the variables:\n",
    "    'train_labels', 'val_labels', 'test_labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bddf344387648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T18:36:04.461469Z",
     "start_time": "2025-02-14T18:36:04.457757Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd469d8808618aac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.3__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce992507c9fa2e69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "***\n",
    "## **Task 4** - Datasets and DataLoaders\n",
    "\n",
    "We will now create the Dataset class, which will hold our extracted features and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2060d6e285b40b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## __Assessed Section - 2.4__\n",
    "----------\n",
    "\n",
    "Create an AudioDataSet class, using the template below:\n",
    "- It should take PyTorch tenors holding features and corresponding labels of a dataset as its constructor arguments\n",
    "- It should save these as attributes in the constructor\n",
    "- The \\_\\_getitem\\_\\_ method should take the integer 'i' as its argument, and return both the features and corresponding label for the $i-th$ item from the dataset\n",
    "- The \\_\\_len\\_\\_ method should return an integer, that is equal to the number of data points held in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f4d64036797d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AudioDataSet(Dataset):              # Don't change this line\n",
    "    def __init__(self, features, labels): # Don't change this line\n",
    "        pass # Define the Class constructor here\n",
    "        \n",
    "    def __getitem__(self, i):             # Don't change this line\n",
    "        pass # Define the __getitem__ method here\n",
    " \n",
    "    def __len__(self):                    # Don't change this line\n",
    "        pass # Define the __len__ method here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f56af-8b61-4e19-be0b-242bcc2ed7c1",
   "metadata": {},
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.4__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0917544-e43c-403c-ba5a-4137d21ef9b1",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __Task 5__ - Neural Network Creation\n",
    "\n",
    "We want to make a multi-class classifier, that takes in the time-frequency representation of the audio as input, and predicts the digit that is being spoken.\n",
    "\n",
    "We will do this using a convolutional neural network.\n",
    "\n",
    "First, define a Convolutional Block Class. The Convolutional block will consist of a 1D-Convolutional layer, followed by a ReLU activation function. The convolutional block should process inputs through these two layers, and then return the output.  The number of input channels, output channels, and kernel size should be determined by arguments provided to the class constructor. The convolutional block should __not__ apply any zero-padding. \n",
    "\n",
    "Then, define a Convolutional Neural Network (CNN) Class. It should consist of 8 of the Convolutional blocks defined previously. It should process inputs sequentially through each of the eight blocks.\n",
    "\n",
    "After the eighth block, the network should apply global average pooling, which should apply pooling in the time-dimension only, reducing the output to a single value in the time-dimension, but not applying averaging over the channel or batch dimensions. Finally, a linear layer should be applied, converting the final output to a tensor with the appropriate shape for this multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424826d-f426-40a6-9deb-c0a1e5b52b7a",
   "metadata": {},
   "source": [
    "## __Assessed Section - 2.5__\n",
    "----------\n",
    "\n",
    "Your task it to:\n",
    "- Create the ConvBlock class described above, using the template provided\n",
    "- Create the ConvNet class described above, using the template provided\n",
    "- Create an instance of your ConvNet class, with kernel size of 5. The number of input channels for the first convolutional block should be chosen based on the dimensionality of the time-frequency representation of your data. Otherwise, the number of channels for convolutional layers should be 16. \n",
    "- Process some data through the neural network class, and compare the shapes of the input and output Tensors. You may use data extracted earlier, or you may create random data using torch.randn(N,C,T), where N is the batch size, C is the channel dimensions and T is the number of frames in the time-frequency representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05c367-29fb-4845-a786-0ea53e286680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):                       # Don't change this line\n",
    "    def __init__(self, your_arguments):                 # Add arguments to the constructor method of your Class\n",
    "        super(ConvBlock, self).__init__()               # Don't change this line\n",
    "        pass # Define your class constructor here\n",
    "\n",
    "    def forward(self, x):                               # Don't change this line\n",
    "        # Define forward method here\n",
    "        return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65f92a-af51-46c6-a0eb-497460711a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):                         # Don't change this line\n",
    "    def __init__(self):                                 # Add arguments to the constructor method of your Class\n",
    "        super(ConvNet, self).__init__()                 # Don't change this line\n",
    "        pass # Define your class constructor here\n",
    "\n",
    "    def forward(self, x):                               # Don't change this line\n",
    "        # Define forward method here\n",
    "        return network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d8ee3-d039-4292-91b2-371686b903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your network, as process some input with it as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316993b-16df-443f-8203-007972c5bf02",
   "metadata": {},
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.5__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb76e6-0f64-4f0d-bc8c-72cce7d627c7",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Task 6 - Neural Network Training\n",
    "\n",
    "Now you will train your Neural Network Digit Classifier.\n",
    "\n",
    "The Training Loop. One iteration of the Training Loop should carry out Stochastic Gradient Descent over the full training dataset, in random batches of batch size 50. Iterating over the complete training dataset once is commonly known as 'one training epoch'. You may use the PyTorch DataLoader class when creating the training loop. Each training epoch must use every item in the training dataset exactly once.\n",
    "Use the Adam optimiser, with default settings, available as 'torch.optim.Adam()'. Ensure you pass the model parameters to the optimiser.\n",
    "Use the Cross-Entropy Loss function. You may use the PyTorch cross-entropy loss function class.\n",
    "After each training epoch, save the average loss over the full training subset to a list of losses.\n",
    "\n",
    "The Validation Loop. The Validation Loop should calculate the average loss and accuracy of the model over the whole Validation subset. The validation loss should be saved to a list of validation losses and the validation accuracy should be saved to a list of validation accuracies. \n",
    "\n",
    "The Test Loop. The Test Loop should calculate the average loss and accuracy of the model over the whole test subset. The test loop should also calculate the accuracy for each class, and generate a confusion matrix. You may you sklearn.metrics to create the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f26f6-e5a7-4505-b6f8-a6e03bb52b3e",
   "metadata": {},
   "source": [
    "## __Assessed Section - 2.6__\n",
    "----------\n",
    "\n",
    "Your task it to:\n",
    "- Create the Training, Validation and Test loops described above.\n",
    "- Train your neural network for 100 epochs, calculating the validation loss and accuracy every 5th epoch.\n",
    "- After training is complete, run the test loop. Display the confusion matrix, and save the resulting figure to a file called 'DigitClassifierConfusion.png'. Include this file with your submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95156892-67f3-4963-bf68-7b5f1d2f09fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e32b9c-f606-4bca-8cf9-268f1144ac73",
   "metadata": {},
   "source": [
    "----------\n",
    "## __End of Assessed Section - 2.6__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b8fb7-8b6a-463b-b0ef-13e8e44fbf56",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __Further Work__\n",
    "\n",
    "The above is sufficient to achieve a mark of 60 for Postgraduate students and 67 for Undergraduate Students. if you wish to go further to get a higher grade, you can try and implement some of the following:\n",
    "\n",
    "- Does the trained classifier generalise to other speakers? Test this using other data from the dataset provided.\n",
    "- Try increasing the diversity of the data used during training, by including more speakers. Train a model using this dataset, and test if this model generalises better to speakers that weren't included in the training set.\n",
    "- Adjust your ConvNet class, so it can receive arguments determining the number of blocks and the activation function used. \n",
    "- There are many choices you could make to change the neural network architecture, or the training process generally. For example, adjusting the number of blocks, changing the time-frequency representation used or adjusting the number of channels in the convolutional layers. Pick one or two of these and run experiments to compare how making changes to these influences the performance of the trained neural network model. Write in a markdown cell a summary of your findings and any conclusions you can draw about the adjustments you made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09f6ea-8374-4e79-bbd4-b5aba6521e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
